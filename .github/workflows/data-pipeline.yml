name: data-pipeline-workflow

on:
  push: # run on push
  schedule:
    - cron: "35 */12 * * *" # run every 12 hours at 35 min mark
  workflow_dispatch:  # manual triggers

jobs:
  run-data-pipeline:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo content
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PERSONAL_ACCESS_TOKEN }}  # Use the PAT instead of the default GITHUB_TOKEN
      - name: Setup python
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'
          cache: 'pip'
      - name: Install dependencies
        run: pip install -r data_pipeline/requirements.txt
      - name: Run data pipeline
        env:
          YT_API_KEY: ${{ secrets.YT_API_KEY }} # import API key
        run: python data_pipeline/data_pipeline.py # run data pipeline
      - name: Check for changes # create env variable indicating if any changes were made
        id: git-check
        run: |
          git config user.name 'github-actions'
          git config user.email 'github-actions@github.com'
          git add .
          git diff --staged --quiet || echo "changes=true" >> $GITHUB_ENV 
      - name: Commit and push if changes
        if: env.changes == 'true' # if changes made push new data to repo
        run: |
          git commit -m "updated video index"
          git push
